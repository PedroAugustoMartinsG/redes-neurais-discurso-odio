{
 "cells": [
  {
   "cell_type": "code",
   "id": "d52d507b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:52:55.577645Z",
     "start_time": "2025-06-23T05:51:44.590311Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import IPython\n",
    "import random\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from gstop import GenerationStopper\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "56f0939fcea24000"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:52:58.401086Z",
     "start_time": "2025-06-23T05:52:57.604705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = kagglehub.dataset_download(\"victorcallejasf/multimodal-hate-speech\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "id": "c82c1ecd66ba20f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\osval\\.cache\\kagglehub\\datasets\\victorcallejasf\\multimodal-hate-speech\\versions\\1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:53:02.072405Z",
     "start_time": "2025-06-23T05:53:01.550966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_gt_path = os.path.join(path, \"MMHS150K_GT.json\")\n",
    "\n",
    "splits = {\n",
    "    \"Treino\": os.path.join(path, \"splits\", \"train_ids.txt\"),\n",
    "    \"Validação\": os.path.join(path, \"splits\", \"val_ids.txt\"),\n",
    "    \"Teste\": os.path.join(path, \"splits\", \"test_ids.txt\"),\n",
    "}\n",
    "\n",
    "with open(json_gt_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def contar_tweets(split_path):\n",
    "    with open(split_path, 'r') as f:\n",
    "        ids = [line.strip() for line in f.readlines()]\n",
    "    return len([id_ for id_ in ids if id_ in data])\n",
    "\n",
    "for nome_split, caminho_split in splits.items():\n",
    "    total_tweets = contar_tweets(caminho_split)\n",
    "    print(f\"{nome_split}: {total_tweets} tweets\")"
   ],
   "id": "9486f6f07c0c9983",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 134823 tweets\n",
      "Validação: 5000 tweets\n",
      "Teste: 10000 tweets\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:53:04.234953Z",
     "start_time": "2025-06-23T05:53:04.157686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove menções, hashtags, URLs e espaços extras do texto.\"\"\"\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "def carregar_dados_singlelabel(split_path):\n",
    "    with open(split_path, 'r') as f:\n",
    "        ids = [line.strip() for line in f.readlines()]\n",
    "    texts = []\n",
    "    label = []\n",
    "    freq = []\n",
    "    for id_ in ids:\n",
    "        if id_ in data:\n",
    "            texts.append(clean_text(data[id_]['tweet_text']))\n",
    "            label_list = data[id_]['labels']  # ex: [0, 1, 1]\n",
    "            _freq = label_list.count(0)\n",
    "            binary_label = False\n",
    "            if _freq <= 1:\n",
    "                binary_label = True\n",
    "            else:\n",
    "                binary_label = False\n",
    "\n",
    "            label.append(binary_label)\n",
    "            freq.append(_freq)\n",
    "\n",
    "            # print(f\"{data[id_]['tweet_text']} - {label_list} - {_freq} - {binary_label}\")\n",
    "\n",
    "    return texts, label, freq\n",
    "\n",
    "# Como o modelo usado é pré-treinado, utilizaremos apenas o conjunto de teste\n",
    "target_texts, target_labels, target_freq = carregar_dados_singlelabel(splits['Teste'])\n",
    "\n",
    "true_count = sum(target_labels)\n",
    "false_count = len(target_labels) - true_count\n",
    "\n",
    "print(f\"Quantidade de elementos com label True: {true_count}\")\n",
    "print(f\"Quantidade de elementos com label False: {false_count}\")\n",
    "print(f\"Total de elementos: {len(target_labels)}\")\n",
    "print(f\"Porcentagem True: {true_count/len(target_labels)*100:.1f}%\")\n",
    "print(f\"Porcentagem False: {false_count/len(target_labels)*100:.1f}%\")"
   ],
   "id": "ac62acfbc89c356d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de elementos com label True: 5001\n",
      "Quantidade de elementos com label False: 4999\n",
      "Total de elementos: 10000\n",
      "Porcentagem True: 50.0%\n",
      "Porcentagem False: 50.0%\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T19:00:34.283616Z",
     "start_time": "2025-06-22T19:00:34.280287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_tweet = 55\n",
    "print(f\"{target_texts[test_tweet]}: Hate: {target_labels[test_tweet]} Freq: {target_freq[test_tweet]}\")"
   ],
   "id": "dbf04116c87f14cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vince wanna say “what do you want from me nigger” so bad: Hate: True Freq: 1\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:53:08.328815Z",
     "start_time": "2025-06-23T05:53:08.315660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Como o modelo é muito pesado e demora muito tempo para classificar os tweets optamos por criar um subset\n",
    "\n",
    "# Cria um subset mantendo proporção específica de True/False\n",
    "def create_proportional_subset(texts, labels, freq, size, prop_false, prop_true):\n",
    "\n",
    "    # Separar índices por label\n",
    "    false_indices = [i for i, label in enumerate(labels) if label == False]\n",
    "    true_indices = [i for i, label in enumerate(labels) if label == True]\n",
    "\n",
    "    # Calcular quantos elementos de cada tipo\n",
    "    n_false = int(size * prop_false)\n",
    "    n_true = int(size * prop_true)\n",
    "\n",
    "    # Ajustar se não houver elementos suficientes\n",
    "    n_false = min(n_false, len(false_indices))\n",
    "    n_true = min(n_true, len(true_indices))\n",
    "\n",
    "    print(f\"Disponível - False: {len(false_indices)}, True: {len(true_indices)}\")\n",
    "    print(f\"Selecionando - False: {n_false}, True: {n_true}\")\n",
    "\n",
    "    # Selecionar aleatoriamente os índices\n",
    "    selected_false = random.sample(false_indices, n_false)\n",
    "    selected_true = random.sample(true_indices, n_true)\n",
    "\n",
    "    # Combinar todos os índices selecionados\n",
    "    selected_indices = selected_false + selected_true\n",
    "\n",
    "    # Criar subsets\n",
    "    subset_texts = [texts[i] for i in selected_indices]\n",
    "    subset_labels = [labels[i] for i in selected_indices]\n",
    "    subset_freq = [freq[i] for i in selected_indices]\n",
    "\n",
    "    return subset_texts, subset_labels, subset_freq\n",
    "\n",
    "# Exemplo de uso com seus dados\n",
    "X = 10000   # Tamanho do subset desejado\n",
    "\n",
    "# Proporção desejada\n",
    "Z = 0.5  # Não Hate\n",
    "Y = 0.5  # Hate\n",
    "\n",
    "# Criar subset\n",
    "subset_texts, subset_labels, subset_freq = create_proportional_subset(\n",
    "    target_texts, target_labels, target_freq, X, Z, Y\n",
    ")\n",
    "\n",
    "# Verificar resultado\n",
    "print(f\"\\n=== RESULTADO DO SUBSET ===\")\n",
    "print(f\"Subset criado com {len(subset_texts)} elementos\")\n",
    "print(f\"False: {subset_labels.count(False)} ({subset_labels.count(False)/len(subset_labels)*100:.1f}%)\")\n",
    "print(f\"True: {subset_labels.count(True)} ({subset_labels.count(True)/len(subset_labels)*100:.1f}%)\")"
   ],
   "id": "d864954d3673ec3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disponível - False: 4999, True: 5001\n",
      "Selecionando - False: 4999, True: 5000\n",
      "\n",
      "=== RESULTADO DO SUBSET ===\n",
      "Subset criado com 9999 elementos\n",
      "False: 4999 (50.0%)\n",
      "True: 5000 (50.0%)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T20:40:33.902240Z",
     "start_time": "2025-06-22T20:40:33.894657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_tweet = 4\n",
    "print(f\"{subset_texts[test_tweet]}: Hate: {subset_labels[test_tweet]} Freq: {subset_freq[test_tweet]}\")"
   ],
   "id": "c2ad9c2ad47ee9b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nigga whaat: Hate: False Freq: 2\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "id": "23e19306",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:53:30.029969Z",
     "start_time": "2025-06-23T05:53:17.016323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuração do modelo\n",
    "torch.cuda.empty_cache()\n",
    "model_name = \"irlab-udc/Llama-3-8B-Distil-MetaHate\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/llama-3-8b-Instruct-bnb-4bit\")\n",
    "model = PeftModel.from_pretrained(base_model, \"irlab-udc/Llama-3-8B-Distil-MetaHate\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ],
   "id": "2a6023b2d5cfc1a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:53:34.831240Z",
     "start_time": "2025-06-23T05:53:32.376396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Otimizacões\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ],
   "id": "9c3b488a89440301",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "49fa4f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:53:36.130002Z",
     "start_time": "2025-06-23T05:53:36.121928Z"
    }
   },
   "source": [
    "class JSONStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_tokens = ['}']\n",
    "        self.stop_token_ids = [tokenizer.encode(token, add_special_tokens=False)[0] for token in self.stop_tokens]\n",
    "    \n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Verifica se o último token gerado é '}'\n",
    "        if input_ids[0][-1] in self.stop_token_ids:\n",
    "            return True\n",
    "        return False"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "3f902857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:53:37.950698Z",
     "start_time": "2025-06-23T05:53:37.945749Z"
    }
   },
   "source": [
    "def create_hate_speech_prompt(message):\n",
    "    \"\"\"Cria o prompt estruturado para classificação simples de discurso de ódio\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You must analyze if a social media message is hateful or not and tell me your decision. You must always reply with only a JSON containing one field 'hate_speech' including a Boolean value (\"True\" for hate speech messages, \"False\" for neutral ones). Do not include text outside the JSON.\n",
    "\n",
    "This is the definition of hate speech: \"language characterized by offensive, derogatory, humiliating, or insulting discourse that promotes violence, discrimination, or hostility towards individuals or groups based on attributes such as race, religion, ethnicity, or gender\".\n",
    "\n",
    "The input format is:\n",
    "\n",
    "Classify:<Message><input query></Message>.\n",
    "The output format is:\n",
    "\n",
    "    {\n",
    "        \"hate_speech\": \"<Boolean>\"\n",
    "    }\"\"\"\n",
    "    \n",
    "    user_input = f\"Classify:<Message>{message}</Message>\"\n",
    "    \n",
    "    return f\"{system_prompt}\\n{user_input}\""
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:53:45.441885Z",
     "start_time": "2025-06-23T05:53:40.278031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_texts(texts):\n",
    "    \"\"\"\n",
    "    Tokeniza uma lista de textos mantendo a ordem\n",
    "    Retorna uma lista de dicionários com tensores no dispositivo correto\n",
    "    \"\"\"\n",
    "    tokenized_list = []\n",
    "\n",
    "    for text in tqdm(texts, desc='Tokenizando textos'):\n",
    "        # Cria o prompt estruturado (mesma função do seu código)\n",
    "        prompt = create_hate_speech_prompt(text)\n",
    "\n",
    "        # Tokeniza seguindo o mesmo padrão\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Move para o dispositivo correto\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Adiciona à lista mantendo a ordem\n",
    "        tokenized_list.append(inputs)\n",
    "\n",
    "    return tokenized_list\n",
    "\n",
    "tokenized_tweets = tokenize_texts(subset_texts)\n"
   ],
   "id": "3a2e6928b0aeb87e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizando textos:   0%|          | 0/9999 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "368886a6fb8a47ef9a596eeb28a362fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T05:53:49.019350Z",
     "start_time": "2025-06-23T05:53:49.012917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_model_outputs(tokenized_inputs):\n",
    "    \"\"\"\n",
    "    Gera outputs do modelo para cada input tokenizado\n",
    "    Retorna uma lista de outputs brutos\n",
    "    \"\"\"\n",
    "    outputs_list = []\n",
    "    stopping_criteria = StoppingCriteriaList([JSONStoppingCriteria(tokenizer)])\n",
    "\n",
    "    for inputs in tqdm(tokenized_inputs, desc='Gerando outputs do modelo'):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=15,\n",
    "                temperature=0.3,\n",
    "                stopping_criteria=stopping_criteria\n",
    "            )\n",
    "\n",
    "        outputs_list.append({\n",
    "            'outputs': outputs,\n",
    "            'input_length': inputs['input_ids'].shape[1]\n",
    "        })\n",
    "\n",
    "    return outputs_list\n",
    "\n",
    "def generate_model_outputs_optimized(tokenized_inputs):\n",
    "    outputs_list = []\n",
    "    stopping_criteria = StoppingCriteriaList([JSONStoppingCriteria(tokenizer)])\n",
    "\n",
    "    for inputs in tqdm(tokenized_inputs, desc='Gerando outputs do modelo'):\n",
    "        with torch.no_grad():\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=15,\n",
    "                    temperature=0.6,\n",
    "                    stopping_criteria=stopping_criteria,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "\n",
    "        outputs_list.append({\n",
    "            'outputs': outputs,\n",
    "            'input_length': inputs['input_ids'].shape[1]\n",
    "        })\n",
    "\n",
    "        if len(outputs_list) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return outputs_list"
   ],
   "id": "88033193c8322de7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:05:21.315854Z",
     "start_time": "2025-06-23T05:53:52.520455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subset_tokenized_tweets = tokenized_tweets\n",
    "\n",
    "# Chama a função com o subset\n",
    "model_outputs = generate_model_outputs_optimized(subset_tokenized_tweets)\n",
    "print(f\"Gerados {len(model_outputs)} outputs do modelo\")"
   ],
   "id": "ae0f07236a8b2f47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gerando outputs do modelo:   0%|          | 0/9999 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68d728deb68f4069ab9aa8d7065bddc6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerados 9999 outputs do modelo\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:34:39.544147Z",
     "start_time": "2025-06-23T12:34:38.800814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def decode_model_responses(model_outputs):\n",
    "    \"\"\"\n",
    "    Decodifica os outputs do modelo em texto\n",
    "    Retorna uma lista de respostas em texto\n",
    "    \"\"\"\n",
    "    decoded_responses = []\n",
    "\n",
    "    for output_data in tqdm(model_outputs, desc='Decodificando respostas'):\n",
    "        # Decodifica apenas a parte nova (sem o prompt)\n",
    "        response = tokenizer.decode(\n",
    "            output_data['outputs'][0][output_data['input_length']:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        decoded_responses.append(response)\n",
    "\n",
    "    return decoded_responses\n",
    "\n",
    "# Uso na segunda célula\n",
    "decoded_responses = decode_model_responses(model_outputs)\n",
    "print(f\"Decodificadas {len(decoded_responses)} respostas\")\n",
    "print(\"Exemplo de resposta:\", decoded_responses[0])"
   ],
   "id": "b18b83d8cc42bf18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decodificando respostas:   0%|          | 0/9999 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f515ad3cf8dc4b059586f64e1b4594a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decodificadas 9999 respostas\n",
      "Exemplo de resposta:     {\n",
      "        \"hate_speech\": \"True\"\n",
      "    }\n",
      "Class\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:34:43.669650Z",
     "start_time": "2025-06-23T12:34:43.613715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_json_responses(decoded_responses):\n",
    "    \"\"\"\n",
    "    Versão com regex para extrair JSON mais robustamente\n",
    "    Inclui tratamento para JSONs válidos mas sem a chave esperada\n",
    "    \"\"\"\n",
    "    parsed_results = []\n",
    "\n",
    "    # Padrão regex para encontrar JSON válido\n",
    "    json_pattern = re.compile(r'\\{[^{}]*\"hate_speech\"\\s*:\\s*\"(True|False)\"[^{}]*\\}')\n",
    "\n",
    "    for i, response in enumerate(tqdm(decoded_responses, desc='Parseando JSON com regex')):\n",
    "        try:\n",
    "            # Tenta encontrar o padrão JSON\n",
    "            match = json_pattern.search(response)\n",
    "            if match:\n",
    "                clean_response = match.group(0)\n",
    "            else:\n",
    "                # Fallback: remove texto após último '}'\n",
    "                last_brace_index = response.rfind('}')\n",
    "                if last_brace_index != -1:\n",
    "                    clean_response = response[:last_brace_index+1]\n",
    "                else:\n",
    "                    clean_response = response\n",
    "\n",
    "            json_result = json.loads(clean_response)\n",
    "\n",
    "            # Verificar se a chave 'hate_speech' existe\n",
    "            if 'hate_speech' in json_result:\n",
    "                parsed_results.append({\n",
    "                    'index': i,\n",
    "                    'response': clean_response,\n",
    "                    'hate_speech': json_result['hate_speech'],\n",
    "                    'parsed_successfully': True,\n",
    "                    'error': None\n",
    "                })\n",
    "            else:\n",
    "                # JSON válido mas sem a chave esperada\n",
    "                print(json_result)\n",
    "                parsed_results.append({\n",
    "                    'index': i,\n",
    "                    'response': clean_response,\n",
    "                    'hate_speech': None,\n",
    "                    'parsed_successfully': False,\n",
    "                    'error': f'JSON válido mas chave \"hate_speech\" não encontrada. Chaves disponíveis: {list(json_result.keys())}'\n",
    "                })\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            parsed_results.append({\n",
    "                'index': i,\n",
    "                'response': response,\n",
    "                'hate_speech': None,\n",
    "                'parsed_successfully': False,\n",
    "                'error': f'JSON parsing failed: {str(e)}'\n",
    "            })\n",
    "\n",
    "    return parsed_results\n",
    "\n",
    "# Uso na terceira célula\n",
    "final_results = parse_json_responses(decoded_responses)\n",
    "\n",
    "# Mostra estatísticas\n",
    "successful_parses = sum(1 for r in final_results if r['parsed_successfully'])\n",
    "hate_speech_count = sum(1 for r in final_results if r['parsed_successfully'] and r['hate_speech'])\n",
    "\n",
    "print(f\"Parsing bem-sucedido: {successful_parses}/{len(final_results)}\")\n",
    "print(f\"Discurso de ódio detectado: {hate_speech_count}/{successful_parses}\")"
   ],
   "id": "e8ad4d7bddff8372",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parseando JSON com regex:   0%|          | 0/9999 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "324d7a1939374f6db450fa8db333d730"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing bem-sucedido: 9065/9999\n",
      "Discurso de ódio detectado: 9065/9065\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Análise dos resultados",
   "id": "b4905788792178aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:34:46.885696Z",
     "start_time": "2025-06-23T12:34:46.881183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_by_category(df):\n",
    "    \"\"\"\n",
    "    Analisa performance separadamente para True e False labels\n",
    "    \"\"\"\n",
    "    valid_df = df[df['ModelOutputLabel'].notna()].copy()\n",
    "\n",
    "    print(f\"\\n=== ANÁLISE POR CATEGORIA ===\")\n",
    "\n",
    "    # Para labels True (hate speech)\n",
    "    true_labels = valid_df[valid_df['RealLabel'] == True]\n",
    "    if len(true_labels) > 0:\n",
    "        true_correct = (true_labels['RealLabel'] == true_labels['ModelOutputLabel']).sum()\n",
    "        true_accuracy = true_correct / len(true_labels)\n",
    "        print(f\"Hate Speech (True) - Total: {len(true_labels)}, Corretos: {true_correct}, Acurácia: {true_accuracy:.4f}\")\n",
    "\n",
    "    # Para labels False (não hate speech)\n",
    "    false_labels = valid_df[valid_df['RealLabel'] == False]\n",
    "    if len(false_labels) > 0:\n",
    "        false_correct = (false_labels['RealLabel'] == false_labels['ModelOutputLabel']).sum()\n",
    "        false_accuracy = false_correct / len(false_labels)\n",
    "        print(f\"Não Hate Speech (False) - Total: {len(false_labels)}, Corretos: {false_correct}, Acurácia: {false_accuracy:.4f}\")"
   ],
   "id": "f28016487956f15c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:34:48.560619Z",
     "start_time": "2025-06-23T12:34:48.554007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Análise mais detalhada\n",
    "def analyze_model_performance(df):\n",
    "    \"\"\"\n",
    "    Analisa a performance do modelo de forma detalhada\n",
    "    \"\"\"\n",
    "    # Filtrar apenas predições válidas\n",
    "    valid_df = df[df['ModelOutputLabel'].notna()].copy()\n",
    "\n",
    "    if len(valid_df) == 0:\n",
    "        print(\"Nenhuma predição válida encontrada!\")\n",
    "        return\n",
    "\n",
    "    # Calcular métricas básicas\n",
    "    correct = (valid_df['RealLabel'] == valid_df['ModelOutputLabel']).sum()\n",
    "    total = len(valid_df)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Matriz de confusão manual\n",
    "    true_positive = ((valid_df['RealLabel'] == True) & (valid_df['ModelOutputLabel'] == True)).sum()\n",
    "    true_negative = ((valid_df['RealLabel'] == False) & (valid_df['ModelOutputLabel'] == False)).sum()\n",
    "    false_positive = ((valid_df['RealLabel'] == False) & (valid_df['ModelOutputLabel'] == True)).sum()\n",
    "    false_negative = ((valid_df['RealLabel'] == True) & (valid_df['ModelOutputLabel'] == False)).sum()\n",
    "\n",
    "    print(f\"\\n=== ANÁLISE DETALHADA DE PERFORMANCE ===\")\n",
    "    print(f\"Total de amostras válidas: {total}\")\n",
    "    print(f\"Acurácia: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "    print(f\"\\n=== MATRIZ DE CONFUSÃO ===\")\n",
    "    print(f\"True Positive (TP): {true_positive}\")\n",
    "    print(f\"True Negative (TN): {true_negative}\")\n",
    "    print(f\"False Positive (FP): {false_positive}\")\n",
    "    print(f\"False Negative (FN): {false_negative}\")\n",
    "\n",
    "    # Calcular métricas adicionais\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"\\n=== MÉTRICAS ADICIONAIS ===\")\n",
    "    print(f\"Precisão: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"Recall: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'confusion_matrix': {\n",
    "            'TP': true_positive,\n",
    "            'TN': true_negative,\n",
    "            'FP': false_positive,\n",
    "            'FN': false_negative\n",
    "        }\n",
    "    }"
   ],
   "id": "5ad480af738be089",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T12:34:51.718885Z",
     "start_time": "2025-06-23T12:34:51.662514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_analysis_spreadsheet(target_texts, target_labels, target_freq, final_results, n=250):\n",
    "    \"\"\"\n",
    "    Cria uma planilha com os primeiros n registros dos dados e resultados do modelo\n",
    "    \"\"\"\n",
    "    # Extrair os labels do modelo do final_results\n",
    "    model_output_labels = []\n",
    "    for res in final_results:\n",
    "        if res['parsed_successfully']:\n",
    "            # Convertendo string 'True'/'False' para boolean\n",
    "            model_output_labels.append(res['hate_speech'] == 'True')\n",
    "        else:\n",
    "            model_output_labels.append(None)  # Caso parsing falhe\n",
    "\n",
    "    # Garantir que temos o mesmo tamanho para todos os arrays\n",
    "    min_len = min(len(target_texts), len(target_labels), len(target_freq), len(model_output_labels))\n",
    "\n",
    "    # Ajustar n se necessário\n",
    "    n = min(n, min_len)\n",
    "\n",
    "    # Selecionar os primeiros n exemplos\n",
    "    texts_n = target_texts[:n]\n",
    "    freq_n = target_freq[:n]\n",
    "    real_labels_n = target_labels[:n]\n",
    "    model_labels_n = model_output_labels[:n]\n",
    "\n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Text': texts_n,\n",
    "        'Freq': freq_n,\n",
    "        'RealLabel': real_labels_n,\n",
    "        'ModelOutputLabel': model_labels_n\n",
    "    })\n",
    "\n",
    "    # REMOVER linhas onde ModelOutputLabel é None\n",
    "    df_filtered = df[df['ModelOutputLabel'].notna()].copy()\n",
    "    df_filtered['ModelOutputLabel'] = df_filtered['ModelOutputLabel'].astype(bool)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# Criar a planilha com os 250 primeiros registros\n",
    "df_analysis = create_analysis_spreadsheet(subset_texts, subset_labels, subset_freq, final_results, len(subset_labels))\n",
    "df_analysis['Correct'] = df_analysis['RealLabel'] == df_analysis['ModelOutputLabel']\n",
    "\n",
    "print(f\"Dimensões: {df_analysis.shape[0]} linhas x {df_analysis.shape[1]} colunas\")\n",
    "valid_predictions = df_analysis['ModelOutputLabel'].notna()\n",
    "correct_count = df_analysis[valid_predictions]['Correct'].sum()\n",
    "total_valid = valid_predictions.sum()\n",
    "\n",
    "# Calcular a acurácia\n",
    "accuracy = correct_count / total_valid if total_valid > 0 else 0\n",
    "\n",
    "# Mostrar as primeiras linhas para verificação\n",
    "print(\"\\nPrimeiras 5 linhas:\")\n",
    "print(df_analysis.head())\n",
    "\n",
    "# Mostrar estatísticas básicas\n",
    "print(f\"\\nEstatísticas:\")\n",
    "print(f\"Total de registros: {len(df_analysis)}\")\n",
    "print(f\"Labels reais - True: {df_analysis['RealLabel'].sum()}, False: {(~df_analysis['RealLabel']).sum()}\")\n",
    "print(f\"Labels do modelo - True: {df_analysis['ModelOutputLabel'].sum()}, False: {(~df_analysis['ModelOutputLabel']).sum()}\")\n",
    "print(f\"Registros com parsing bem-sucedido: {df_analysis['ModelOutputLabel'].notna().sum()}\")\n",
    "\n",
    "print(f\"\\n=== ANÁLISE DE PERFORMANCE ===\")\n",
    "print(f\"Total de predições válidas: {total_valid}\")\n",
    "print(f\"Predições corretas: {correct_count}\")\n",
    "print(f\"Predições incorretas: {total_valid - correct_count}\")\n",
    "print(f\"Acurácia: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Exportar para CSV\n",
    "df_analysis.to_csv('dados.csv', index=False)\n",
    "\n",
    "performance_metrics = analyze_model_performance(df_analysis)\n",
    "analyze_by_category(df_analysis)"
   ],
   "id": "ca27416837b3ad19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões: 9065 linhas x 5 colunas\n",
      "\n",
      "Primeiras 5 linhas:\n",
      "                                                Text  Freq  RealLabel  \\\n",
      "0              nigga got a cold welp i’m bout to die     3      False   \n",
      "1        hillbilly superman by the shane givens band     3      False   \n",
      "2  full movie: dark haired babe lucia love toys a...     2      False   \n",
      "3                    shout out to my young nigga lol     2      False   \n",
      "4          ..... can't a nigga feel special for once     3      False   \n",
      "\n",
      "   ModelOutputLabel  Correct  \n",
      "0              True    False  \n",
      "1             False     True  \n",
      "2             False     True  \n",
      "3              True    False  \n",
      "4              True    False  \n",
      "\n",
      "Estatísticas:\n",
      "Total de registros: 9065\n",
      "Labels reais - True: 4526, False: 4539\n",
      "Labels do modelo - True: 7577, False: 1488\n",
      "Registros com parsing bem-sucedido: 9065\n",
      "\n",
      "=== ANÁLISE DE PERFORMANCE ===\n",
      "Total de predições válidas: 9065\n",
      "Predições corretas: 4666\n",
      "Predições incorretas: 4399\n",
      "Acurácia: 0.5147 (51.47%)\n",
      "\n",
      "=== ANÁLISE DETALHADA DE PERFORMANCE ===\n",
      "Total de amostras válidas: 9065\n",
      "Acurácia: 0.5147 (51.47%)\n",
      "\n",
      "=== MATRIZ DE CONFUSÃO ===\n",
      "True Positive (TP): 3852\n",
      "True Negative (TN): 814\n",
      "False Positive (FP): 3725\n",
      "False Negative (FN): 674\n",
      "\n",
      "=== MÉTRICAS ADICIONAIS ===\n",
      "Precisão: 0.5084 (50.84%)\n",
      "Recall: 0.8511 (85.11%)\n",
      "F1-Score: 0.6365\n",
      "\n",
      "=== ANÁLISE POR CATEGORIA ===\n",
      "Hate Speech (True) - Total: 4526, Corretos: 3852, Acurácia: 0.8511\n",
      "Não Hate Speech (False) - Total: 4539, Corretos: 814, Acurácia: 0.1793\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
